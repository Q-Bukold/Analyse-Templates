import nltk 

#df_creep = pd.read_csv("/Users/qbukold/Desktop/Schnulzen/1000_creep.tsv", sep='\t', lineterminator='\n')
#df_hurt = pd.read_csv("/Users/qbukold/Desktop/Schnulzen/1000_hurt.tsv", sep="\t", lineterminator='\n')
#df_abba = pd.read_csv("/Users/qbukold/Desktop/Schnulzen/1000_abba.tsv", sep="\t", lineterminator='\n')
#df_gwtw = pd.read_csv("/Users/qbukold/Desktop/Schnulzen/600_gwtw.tsv", sep="\t", lineterminator='\n')

def append_if_english (list):
    list_new = []
    for line in list:
        content = line.split("\t")
        author = content[0]
        comment = content[1]

        #tokenize comment
        tokens_com = nltk.wordpunct_tokenize(comment)
        print(tokens_com)

        #delete if no english words
        english = 0
        for token in tokens_com:
            if token.lower() in words:
                english += 1
            elif token.lower() in words and len(tokens_com) == 1:
                english += 100
        perc_en = english/len(tokens_com)

        if perc_en > 0.2:
            list_new.append(line)
        return list_new

def load_tsv_list(filename):
    list = []
    with open(filename, 'r') as content:
        for line in content.readlines():
            line = line.strip()
            if line != "":
                list.append(line)
    list.pop(0)
    return list


nltk.download('words')
words = set(nltk.corpus.words.words())

#load tsv as list
creep = load_tsv_list("/Users/qbukold/Desktop/Schnulzen/1000_creep.tsv")
hurt = load_tsv_list("/Users/qbukold/Desktop/Schnulzen/1000_hurt.tsv")
abba = load_tsv_list("/Users/qbukold/Desktop/Schnulzen/1000_abba.tsv")
gwtw = load_tsv_list("/Users/qbukold/Desktop/Schnulzen/600_gwtw.tsv")

## delete if not more than 20% english words OR one english Word
creep = append_if_english(creep)



with open("/Users/qbukold/Desktop/Schnulzen/creep_comments23.tsv", "w") as save_file:
    save_file.write("name\tcomment\n")
    for line in creep:
        if line.split("\t")[1] != "":
            save_file.write(line + "\n")
